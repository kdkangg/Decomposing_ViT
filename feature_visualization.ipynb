{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from util import ActivationDataset\n",
    "from learned_dict import TiedSAE\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from feature_vis import *\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "augment = nn.Sequential(\n",
    "    RepeatBatch(8),\n",
    "    ColorJitter(8),\n",
    "    GausianNoise(8),\n",
    "    Tile(1), Jitter()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"nateraw/vit-base-patch16-224-cifar10\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"nateraw/vit-base-patch16-224-cifar10\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\", split=\"test\")\n",
    "images = dataset[:10][\"img\"]\n",
    "labels = dataset[:10][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        axes[i,j].set_title(labels[i*5+j])\n",
    "        axes[i,j].imshow(images[i*5+j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=images, return_tensors=\"pt\").pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs, output_hidden_states=True)['hidden_states'][-1].cpu().detach().numpy().mean(1).argmax(axis=1)\n",
    "# feature 187 is always the most activated in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = model(inputs, output_hidden_states=True)['hidden_states'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 187 #most activated feature for class 3 cat\n",
    "input_size = 224\n",
    "optimized_input = feature_vis(model, 11,neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 154 #most activated feature after 187 for airplane\n",
    "input_size = 224\n",
    "optimized_input = feature_vis(model, 11,neuron_index, input_size, num_iterations=1000, lr=0.01, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 432 #most activated feature after 187 for class truck\n",
    "input_size = 224\n",
    "optimized_input = feature_vis(model, 11,neuron_index, input_size, num_iterations=1000, lr=0.01, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 86 #most activated feature after 187 for class dog\n",
    "input_size = 224\n",
    "optimized_input = feature_vis(model, 11,neuron_index, input_size, num_iterations=1000, lr=0.01, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sae\n",
    "encoder = torch.randn((7680, 768), device = device) # encoder\n",
    "nn.init.xavier_uniform_(encoder)\n",
    "encoder_bias = torch.zeros(7680, device = device) # encoder bias\n",
    "sae = TiedSAE(encoder, encoder_bias)\n",
    "sae.load_state_dict(torch.load('SAE_models/cifar10/SAE_ratio10_epoch100_lr0.0001.pth'))\n",
    "sae.to_device(device)\n",
    "\n",
    "\n",
    "def top_act(act_path:str, target_layer:int, cls:int, sae,k=30):\n",
    "    datasets = []\n",
    "\n",
    "    dataset = ActivationDataset(f'{act_path}/cifar10_activations_{cls}.h5',f'vit.encoder.layer.{target_layer}.output')\n",
    "    datasets.append(dataset)\n",
    "    dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    sae.to_device(device)\n",
    "    sae.encoder.requires_grad = False\n",
    "    sae.encoder_bias.requires_grad = False\n",
    "\n",
    "    top_activations = []\n",
    "    with torch.no_grad():\n",
    "        for i, (image, activations) in enumerate(data_loader):\n",
    "            activations = activations.to(device)\n",
    "            c = sae.encode(activations)\n",
    "            top_indices = torch.topk(c.mean(dim=1), k, largest=True).indices\n",
    "            top_activations.append(top_indices)\n",
    "    top_activations = torch.cat(top_activations, dim=0)\n",
    "\n",
    "    return top_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in activations:\n",
    "    print(sae.encode(i.unsqueeze(0)).squeeze().mean(0).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 4))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        sns.heatmap(activations[i*5+j][1:,187].reshape(14,14).cpu().detach().numpy(), ax=axes[i,j])\n",
    "        axes[i,j].set_title(labels[i*5+j])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement to ViT-SAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for features most activated for final layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = sae_encoder()\n",
    "encode.encode.weight = nn.Parameter(sae.encoder.data, requires_grad=False)\n",
    "encode.encode.bias = nn.Parameter(sae.encoder_bias.data, requires_grad=False)\n",
    "\n",
    "decode = sae_decoder()\n",
    "decode.decode.weight = nn.Parameter(sae.get_learned_dict().data.T, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vit.encoder.layer[11] = Added_layer(model.vit.encoder.layer[11], encode, decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 6775 #most activated feature for class 3 cat\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_index = 5115 #most activated feature for class 0 airplane\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=False)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 1454\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 2525 #most activated feature for class dog\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=[0, 0.0001, 0.0005, 0.001]\n",
    "for i in lambdas:\n",
    "    neuron_index = 2525 #most activated feature for class dog\n",
    "    input_size = 224\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = i, show_intermediate=False)\n",
    "    print(f'lambda: {i}')\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 4753\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_index = 880\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_by_class = [4775,5266,2792,6775,1189,2525,1454,599,319,3673 ]\n",
    "for n in most_by_class:\n",
    "    neuron_index = n # most for airplane\n",
    "    input_size = 224\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=False)\n",
    "    print(f'feature {n}')\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd most activated for automobile\n",
    "\n",
    "neuron_index = 509\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.00005, show_intermediate=True)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd most activated for horse\n",
    "\n",
    "neuron_index = 7100\n",
    "input_size = 224\n",
    "optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005, show_intermediate=False)\n",
    "plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for features most activated for non-later layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for early layers\n",
    "model = ViTForImageClassification.from_pretrained(\"nateraw/vit-base-patch16-224-cifar10\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[0] = Added_layer(model.vit.encoder.layer[0], encode, decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_path ='activations_cifar10_vit_b'\n",
    "target_layer = 0\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=0, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mid layers\n",
    "model = ViTForImageClassification.from_pretrained(\"nateraw/vit-base-patch16-224-cifar10\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[5] = Added_layer(model.vit.encoder.layer[5], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_vit_b'\n",
    "target_layer = 5\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=5, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vit-dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"facebook/dino-vitb16\")\n",
    "model.eval()\n",
    "\n",
    "sae = TiedSAE(encoder, encoder_bias)\n",
    "sae.load_state_dict(torch.load('SAE_models/cifar10_dino/SAE_ratio10_epoch100_lr0.0001.pth'))\n",
    "sae.to_device(device)\n",
    "\n",
    "encode = sae_encoder()\n",
    "encode.encode.weight = nn.Parameter(sae.encoder.data, requires_grad=False)\n",
    "encode.encode.bias = nn.Parameter(sae.encoder_bias.data, requires_grad=False)\n",
    "\n",
    "decode = sae_decoder()\n",
    "decode.decode.weight = nn.Parameter(sae.get_learned_dict().data.T, requires_grad=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_path ='activations_cifar10_dino_vitb16'\n",
    "target_layer = 11\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vit.encoder.layer[11] = Added_layer(model.vit.encoder.layer[11], encode, decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = list(value_counts.keys())[:30] #most activated feature for class 0 airplane\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"facebook/dino-vitb16\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[0] = Added_layer(model.vit.encoder.layer[0], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_dino_vitb16'\n",
    "target_layer = 0\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=0, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"facebook/dino-vitb16\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[5] = Added_layer(model.vit.encoder.layer[5], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_dino_vitb16'\n",
    "target_layer = 5\n",
    "\n",
    "for cls in range(10):\n",
    "    top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "    value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "    idx = list(value_counts.keys())[:5]\n",
    "    input_size = 224\n",
    "    for neuron_index in idx:\n",
    "        print(f'feature {neuron_index}')\n",
    "        optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=5, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "        plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vitmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most activated features for layer 0, class 0 airplane\n",
    "model = ViTForImageClassification.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[0] = Added_layer(model.vit.encoder.layer[0], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_facebook_vitmae'\n",
    "target_layer = 0\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=0, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most activated features for layer 5, class 0 airplane\n",
    "model = ViTForImageClassification.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[5] = Added_layer(model.vit.encoder.layer[5], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_facebook_vitmae'\n",
    "target_layer = 5\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=5, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most activated features for layer 11, class 0 airplane\n",
    "model = ViTForImageClassification.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.eval()\n",
    "model.vit.encoder.layer[11] = Added_layer(model.vit.encoder.layer[11], encode, decode)\n",
    "\n",
    "act_path ='activations_cifar10_facebook_vitmae'\n",
    "target_layer = 11\n",
    "cls = 0\n",
    "top_activations = top_act(act_path, target_layer, cls, sae)\n",
    "value_counts = Counter(top_activations.cpu().numpy().flatten().tolist())\n",
    "\n",
    "idx = list(value_counts.keys())[:30]\n",
    "input_size = 224\n",
    "for neuron_index in idx:\n",
    "    print(f'feature {neuron_index}')\n",
    "    optimized_input = sae_feature_vis(model, neuron_index, input_size, sae_layer=11, num_iterations=1000, lr=0.1, device=device, augment=augment, lambda_tv = 0.0005)\n",
    "    plt.imshow(optimized_input[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
